{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "000_cuda_deviceQuery.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNRIcoUUisoWZju6/OWHoee",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AEW2015/fpga-colab/blob/main/HPC_for_Versal/Cuda/000_deviceQuery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CUDA device query example with cpp/cu application!\n"
      ],
      "metadata": {
        "id": "K7bpKvIQ-3JK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSNfyA4S8uMV",
        "outputId": "a2cde47a-06c6-431a-cfbb-6cde6fe0675c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Aug 31 03:40:46 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/NVIDIA/cuda-samples.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNj0A_JF_Fgx",
        "outputId": "d67de56b-67de-4916-efe3-b54dbfbd4ec2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'cuda-samples'...\n",
            "remote: Enumerating objects: 9693, done.\u001b[K\n",
            "remote: Total 9693 (delta 0), reused 0 (delta 0), pack-reused 9693\u001b[K\n",
            "Receiving objects: 100% (9693/9693), 122.53 MiB | 16.09 MiB/s, done.\n",
            "Resolving deltas: 100% (7840/7840), done.\n",
            "Checking out files: 100% (3612/3612), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/cuda-samples/\n",
        "!git checkout tags/v11.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcUdCL0j_YCp",
        "outputId": "eaec9560-4f9b-4ca5-a7ea-d95adf30f4ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cuda-samples\n",
            "Previous HEAD position was b882fa0 -- Add freeglut and glew64 libs for simpleGL -- delete freeimage libs and headers, user needs to install it for building samples depending on it\n",
            "HEAD is now at c4e2869 add multi-warp cooperative groups based reduction kernel in reduction sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/cuda-samples/Samples/deviceQuery/\n",
        "!make \n",
        "!./deviceQuery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkXwVuXu_fA4",
        "outputId": "4875e7e4-214a-463f-f24e-d94cdf05b281"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cuda-samples/Samples/deviceQuery\n",
            "/usr/local/cuda/bin/nvcc -ccbin g++ -I../../Common  -m64    -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o deviceQuery.o -c deviceQuery.cpp\n",
            "/usr/local/cuda/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -o deviceQuery deviceQuery.o \n",
            "mkdir -p ../../bin/x86_64/linux/release\n",
            "cp deviceQuery ../../bin/x86_64/linux/release\n",
            "./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla T4\"\n",
            "  CUDA Driver Version / Runtime Version          11.2 / 11.1\n",
            "  CUDA Capability Major/Minor version number:    7.5\n",
            "  Total amount of global memory:                 15110 MBytes (15843721216 bytes)\n",
            "  (40) Multiprocessors, ( 64) CUDA Cores/MP:     2560 CUDA Cores\n",
            "  GPU Max Clock rate:                            1590 MHz (1.59 GHz)\n",
            "  Memory Clock rate:                             5001 Mhz\n",
            "  Memory Bus Width:                              256-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total shared memory per multiprocessor:        65536 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  1024\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Managed Memory:                Yes\n",
            "  Device supports Compute Preemption:            Yes\n",
            "  Supports Cooperative Kernel Launch:            Yes\n",
            "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.2, CUDA Runtime Version = 11.1, NumDevs = 1\n",
            "Result = PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* You can change the SMS value in the makefile to match the CUDA Capability Major/Minor version number (7.5 => 75)\n",
        "* example on line 275: `SMS = 75`"
      ],
      "metadata": {
        "id": "gubQ74W9AN5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat /content/cuda-samples/Samples/deviceQuery/deviceQuery.cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXwqan8i_q9J",
        "outputId": "5401d25c-4db1-40ca-98b0-591fece6d4a9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/* Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n",
            " *\n",
            " * Redistribution and use in source and binary forms, with or without\n",
            " * modification, are permitted provided that the following conditions\n",
            " * are met:\n",
            " *  * Redistributions of source code must retain the above copyright\n",
            " *    notice, this list of conditions and the following disclaimer.\n",
            " *  * Redistributions in binary form must reproduce the above copyright\n",
            " *    notice, this list of conditions and the following disclaimer in the\n",
            " *    documentation and/or other materials provided with the distribution.\n",
            " *  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
            " *    contributors may be used to endorse or promote products derived\n",
            " *    from this software without specific prior written permission.\n",
            " *\n",
            " * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
            " * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
            " * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
            " * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
            " * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
            " * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
            " * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
            " * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
            " * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
            " * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
            " * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            " */\n",
            "\n",
            "/* This sample queries the properties of the CUDA devices present in the system\n",
            " * via CUDA Runtime API. */\n",
            "\n",
            "// std::system includes\n",
            "\n",
            "#include <cuda_runtime.h>\n",
            "#include <helper_cuda.h>\n",
            "\n",
            "#include <iostream>\n",
            "#include <memory>\n",
            "#include <string>\n",
            "\n",
            "int *pArgc = NULL;\n",
            "char **pArgv = NULL;\n",
            "\n",
            "#if CUDART_VERSION < 5000\n",
            "\n",
            "// CUDA-C includes\n",
            "#include <cuda.h>\n",
            "\n",
            "// This function wraps the CUDA Driver API into a template function\n",
            "template <class T>\n",
            "inline void getCudaAttribute(T *attribute, CUdevice_attribute device_attribute,\n",
            "                             int device) {\n",
            "  CUresult error = cuDeviceGetAttribute(attribute, device_attribute, device);\n",
            "\n",
            "  if (CUDA_SUCCESS != error) {\n",
            "    fprintf(\n",
            "        stderr,\n",
            "        \"cuSafeCallNoSync() Driver API error = %04d from file <%s>, line %i.\\n\",\n",
            "        error, __FILE__, __LINE__);\n",
            "\n",
            "    exit(EXIT_FAILURE);\n",
            "  }\n",
            "}\n",
            "\n",
            "#endif /* CUDART_VERSION < 5000 */\n",
            "\n",
            "////////////////////////////////////////////////////////////////////////////////\n",
            "// Program main\n",
            "////////////////////////////////////////////////////////////////////////////////\n",
            "int main(int argc, char **argv) {\n",
            "  pArgc = &argc;\n",
            "  pArgv = argv;\n",
            "\n",
            "  printf(\"%s Starting...\\n\\n\", argv[0]);\n",
            "  printf(\n",
            "      \" CUDA Device Query (Runtime API) version (CUDART static linking)\\n\\n\");\n",
            "\n",
            "  int deviceCount = 0;\n",
            "  cudaError_t error_id = cudaGetDeviceCount(&deviceCount);\n",
            "\n",
            "  if (error_id != cudaSuccess) {\n",
            "    printf(\"cudaGetDeviceCount returned %d\\n-> %s\\n\",\n",
            "           static_cast<int>(error_id), cudaGetErrorString(error_id));\n",
            "    printf(\"Result = FAIL\\n\");\n",
            "    exit(EXIT_FAILURE);\n",
            "  }\n",
            "\n",
            "  // This function call returns 0 if there are no CUDA capable devices.\n",
            "  if (deviceCount == 0) {\n",
            "    printf(\"There are no available device(s) that support CUDA\\n\");\n",
            "  } else {\n",
            "    printf(\"Detected %d CUDA Capable device(s)\\n\", deviceCount);\n",
            "  }\n",
            "\n",
            "  int dev, driverVersion = 0, runtimeVersion = 0;\n",
            "\n",
            "  for (dev = 0; dev < deviceCount; ++dev) {\n",
            "    cudaSetDevice(dev);\n",
            "    cudaDeviceProp deviceProp;\n",
            "    cudaGetDeviceProperties(&deviceProp, dev);\n",
            "\n",
            "    printf(\"\\nDevice %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n",
            "\n",
            "    // Console log\n",
            "    cudaDriverGetVersion(&driverVersion);\n",
            "    cudaRuntimeGetVersion(&runtimeVersion);\n",
            "    printf(\"  CUDA Driver Version / Runtime Version          %d.%d / %d.%d\\n\",\n",
            "           driverVersion / 1000, (driverVersion % 100) / 10,\n",
            "           runtimeVersion / 1000, (runtimeVersion % 100) / 10);\n",
            "    printf(\"  CUDA Capability Major/Minor version number:    %d.%d\\n\",\n",
            "           deviceProp.major, deviceProp.minor);\n",
            "\n",
            "    char msg[256];\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "    sprintf_s(msg, sizeof(msg),\n",
            "             \"  Total amount of global memory:                 %.0f MBytes \"\n",
            "             \"(%llu bytes)\\n\",\n",
            "             static_cast<float>(deviceProp.totalGlobalMem / 1048576.0f),\n",
            "             (unsigned long long)deviceProp.totalGlobalMem);\n",
            "#else\n",
            "    snprintf(msg, sizeof(msg),\n",
            "             \"  Total amount of global memory:                 %.0f MBytes \"\n",
            "             \"(%llu bytes)\\n\",\n",
            "             static_cast<float>(deviceProp.totalGlobalMem / 1048576.0f),\n",
            "             (unsigned long long)deviceProp.totalGlobalMem);\n",
            "#endif\n",
            "    printf(\"%s\", msg);\n",
            "\n",
            "    printf(\"  (%2d) Multiprocessors, (%3d) CUDA Cores/MP:     %d CUDA Cores\\n\",\n",
            "           deviceProp.multiProcessorCount,\n",
            "           _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor),\n",
            "           _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor) *\n",
            "               deviceProp.multiProcessorCount);\n",
            "    printf(\n",
            "        \"  GPU Max Clock rate:                            %.0f MHz (%0.2f \"\n",
            "        \"GHz)\\n\",\n",
            "        deviceProp.clockRate * 1e-3f, deviceProp.clockRate * 1e-6f);\n",
            "\n",
            "#if CUDART_VERSION >= 5000\n",
            "    // This is supported in CUDA 5.0 (runtime API device properties)\n",
            "    printf(\"  Memory Clock rate:                             %.0f Mhz\\n\",\n",
            "           deviceProp.memoryClockRate * 1e-3f);\n",
            "    printf(\"  Memory Bus Width:                              %d-bit\\n\",\n",
            "           deviceProp.memoryBusWidth);\n",
            "\n",
            "    if (deviceProp.l2CacheSize) {\n",
            "      printf(\"  L2 Cache Size:                                 %d bytes\\n\",\n",
            "             deviceProp.l2CacheSize);\n",
            "    }\n",
            "\n",
            "#else\n",
            "    // This only available in CUDA 4.0-4.2 (but these were only exposed in the\n",
            "    // CUDA Driver API)\n",
            "    int memoryClock;\n",
            "    getCudaAttribute<int>(&memoryClock, CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE,\n",
            "                          dev);\n",
            "    printf(\"  Memory Clock rate:                             %.0f Mhz\\n\",\n",
            "           memoryClock * 1e-3f);\n",
            "    int memBusWidth;\n",
            "    getCudaAttribute<int>(&memBusWidth,\n",
            "                          CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH, dev);\n",
            "    printf(\"  Memory Bus Width:                              %d-bit\\n\",\n",
            "           memBusWidth);\n",
            "    int L2CacheSize;\n",
            "    getCudaAttribute<int>(&L2CacheSize, CU_DEVICE_ATTRIBUTE_L2_CACHE_SIZE, dev);\n",
            "\n",
            "    if (L2CacheSize) {\n",
            "      printf(\"  L2 Cache Size:                                 %d bytes\\n\",\n",
            "             L2CacheSize);\n",
            "    }\n",
            "\n",
            "#endif\n",
            "\n",
            "    printf(\n",
            "        \"  Maximum Texture Dimension Size (x,y,z)         1D=(%d), 2D=(%d, \"\n",
            "        \"%d), 3D=(%d, %d, %d)\\n\",\n",
            "        deviceProp.maxTexture1D, deviceProp.maxTexture2D[0],\n",
            "        deviceProp.maxTexture2D[1], deviceProp.maxTexture3D[0],\n",
            "        deviceProp.maxTexture3D[1], deviceProp.maxTexture3D[2]);\n",
            "    printf(\n",
            "        \"  Maximum Layered 1D Texture Size, (num) layers  1D=(%d), %d layers\\n\",\n",
            "        deviceProp.maxTexture1DLayered[0], deviceProp.maxTexture1DLayered[1]);\n",
            "    printf(\n",
            "        \"  Maximum Layered 2D Texture Size, (num) layers  2D=(%d, %d), %d \"\n",
            "        \"layers\\n\",\n",
            "        deviceProp.maxTexture2DLayered[0], deviceProp.maxTexture2DLayered[1],\n",
            "        deviceProp.maxTexture2DLayered[2]);\n",
            "\n",
            "    printf(\"  Total amount of constant memory:               %zu bytes\\n\",\n",
            "           deviceProp.totalConstMem);\n",
            "    printf(\"  Total amount of shared memory per block:       %zu bytes\\n\",\n",
            "           deviceProp.sharedMemPerBlock);\n",
            "    printf(\"  Total shared memory per multiprocessor:        %zu bytes\\n\",\n",
            "           deviceProp.sharedMemPerMultiprocessor);\n",
            "    printf(\"  Total number of registers available per block: %d\\n\",\n",
            "           deviceProp.regsPerBlock);\n",
            "    printf(\"  Warp size:                                     %d\\n\",\n",
            "           deviceProp.warpSize);\n",
            "    printf(\"  Maximum number of threads per multiprocessor:  %d\\n\",\n",
            "           deviceProp.maxThreadsPerMultiProcessor);\n",
            "    printf(\"  Maximum number of threads per block:           %d\\n\",\n",
            "           deviceProp.maxThreadsPerBlock);\n",
            "    printf(\"  Max dimension size of a thread block (x,y,z): (%d, %d, %d)\\n\",\n",
            "           deviceProp.maxThreadsDim[0], deviceProp.maxThreadsDim[1],\n",
            "           deviceProp.maxThreadsDim[2]);\n",
            "    printf(\"  Max dimension size of a grid size    (x,y,z): (%d, %d, %d)\\n\",\n",
            "           deviceProp.maxGridSize[0], deviceProp.maxGridSize[1],\n",
            "           deviceProp.maxGridSize[2]);\n",
            "    printf(\"  Maximum memory pitch:                          %zu bytes\\n\",\n",
            "           deviceProp.memPitch);\n",
            "    printf(\"  Texture alignment:                             %zu bytes\\n\",\n",
            "           deviceProp.textureAlignment);\n",
            "    printf(\n",
            "        \"  Concurrent copy and kernel execution:          %s with %d copy \"\n",
            "        \"engine(s)\\n\",\n",
            "        (deviceProp.deviceOverlap ? \"Yes\" : \"No\"), deviceProp.asyncEngineCount);\n",
            "    printf(\"  Run time limit on kernels:                     %s\\n\",\n",
            "           deviceProp.kernelExecTimeoutEnabled ? \"Yes\" : \"No\");\n",
            "    printf(\"  Integrated GPU sharing Host Memory:            %s\\n\",\n",
            "           deviceProp.integrated ? \"Yes\" : \"No\");\n",
            "    printf(\"  Support host page-locked memory mapping:       %s\\n\",\n",
            "           deviceProp.canMapHostMemory ? \"Yes\" : \"No\");\n",
            "    printf(\"  Alignment requirement for Surfaces:            %s\\n\",\n",
            "           deviceProp.surfaceAlignment ? \"Yes\" : \"No\");\n",
            "    printf(\"  Device has ECC support:                        %s\\n\",\n",
            "           deviceProp.ECCEnabled ? \"Enabled\" : \"Disabled\");\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "    printf(\"  CUDA Device Driver Mode (TCC or WDDM):         %s\\n\",\n",
            "           deviceProp.tccDriver ? \"TCC (Tesla Compute Cluster Driver)\"\n",
            "                                : \"WDDM (Windows Display Driver Model)\");\n",
            "#endif\n",
            "    printf(\"  Device supports Unified Addressing (UVA):      %s\\n\",\n",
            "           deviceProp.unifiedAddressing ? \"Yes\" : \"No\");\n",
            "    printf(\"  Device supports Managed Memory:                %s\\n\",\n",
            "           deviceProp.managedMemory ? \"Yes\" : \"No\");\n",
            "    printf(\"  Device supports Compute Preemption:            %s\\n\",\n",
            "           deviceProp.computePreemptionSupported ? \"Yes\" : \"No\");\n",
            "    printf(\"  Supports Cooperative Kernel Launch:            %s\\n\",\n",
            "           deviceProp.cooperativeLaunch ? \"Yes\" : \"No\");\n",
            "    printf(\"  Supports MultiDevice Co-op Kernel Launch:      %s\\n\",\n",
            "           deviceProp.cooperativeMultiDeviceLaunch ? \"Yes\" : \"No\");\n",
            "    printf(\"  Device PCI Domain ID / Bus ID / location ID:   %d / %d / %d\\n\",\n",
            "           deviceProp.pciDomainID, deviceProp.pciBusID, deviceProp.pciDeviceID);\n",
            "\n",
            "    const char *sComputeMode[] = {\n",
            "        \"Default (multiple host threads can use ::cudaSetDevice() with device \"\n",
            "        \"simultaneously)\",\n",
            "        \"Exclusive (only one host thread in one process is able to use \"\n",
            "        \"::cudaSetDevice() with this device)\",\n",
            "        \"Prohibited (no host thread can use ::cudaSetDevice() with this \"\n",
            "        \"device)\",\n",
            "        \"Exclusive Process (many threads in one process is able to use \"\n",
            "        \"::cudaSetDevice() with this device)\",\n",
            "        \"Unknown\",\n",
            "        NULL};\n",
            "    printf(\"  Compute Mode:\\n\");\n",
            "    printf(\"     < %s >\\n\", sComputeMode[deviceProp.computeMode]);\n",
            "  }\n",
            "\n",
            "  // If there are 2 or more GPUs, query to determine whether RDMA is supported\n",
            "  if (deviceCount >= 2) {\n",
            "    cudaDeviceProp prop[64];\n",
            "    int gpuid[64];  // we want to find the first two GPUs that can support P2P\n",
            "    int gpu_p2p_count = 0;\n",
            "\n",
            "    for (int i = 0; i < deviceCount; i++) {\n",
            "      checkCudaErrors(cudaGetDeviceProperties(&prop[i], i));\n",
            "\n",
            "      // Only boards based on Fermi or later can support P2P\n",
            "      if ((prop[i].major >= 2)\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "          // on Windows (64-bit), the Tesla Compute Cluster driver for windows\n",
            "          // must be enabled to support this\n",
            "          && prop[i].tccDriver\n",
            "#endif\n",
            "      ) {\n",
            "        // This is an array of P2P capable GPUs\n",
            "        gpuid[gpu_p2p_count++] = i;\n",
            "      }\n",
            "    }\n",
            "\n",
            "    // Show all the combinations of support P2P GPUs\n",
            "    int can_access_peer;\n",
            "\n",
            "    if (gpu_p2p_count >= 2) {\n",
            "      for (int i = 0; i < gpu_p2p_count; i++) {\n",
            "        for (int j = 0; j < gpu_p2p_count; j++) {\n",
            "          if (gpuid[i] == gpuid[j]) {\n",
            "            continue;\n",
            "          }\n",
            "          checkCudaErrors(\n",
            "              cudaDeviceCanAccessPeer(&can_access_peer, gpuid[i], gpuid[j]));\n",
            "          printf(\"> Peer access from %s (GPU%d) -> %s (GPU%d) : %s\\n\",\n",
            "                 prop[gpuid[i]].name, gpuid[i], prop[gpuid[j]].name, gpuid[j],\n",
            "                 can_access_peer ? \"Yes\" : \"No\");\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "\n",
            "  // csv masterlog info\n",
            "  // *****************************\n",
            "  // exe and CUDA driver name\n",
            "  printf(\"\\n\");\n",
            "  std::string sProfileString = \"deviceQuery, CUDA Driver = CUDART\";\n",
            "  char cTemp[16];\n",
            "\n",
            "  // driver version\n",
            "  sProfileString += \", CUDA Driver Version = \";\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "  sprintf_s(cTemp, 10, \"%d.%d\", driverVersion/1000, (driverVersion%100)/10);\n",
            "#else\n",
            "  snprintf(cTemp, sizeof(cTemp), \"%d.%d\", driverVersion / 1000,\n",
            "           (driverVersion % 100) / 10);\n",
            "#endif\n",
            "  sProfileString += cTemp;\n",
            "\n",
            "  // Runtime version\n",
            "  sProfileString += \", CUDA Runtime Version = \";\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "  sprintf_s(cTemp, 10, \"%d.%d\", runtimeVersion/1000, (runtimeVersion%100)/10);\n",
            "#else\n",
            "  snprintf(cTemp, sizeof(cTemp), \"%d.%d\", runtimeVersion / 1000,\n",
            "           (runtimeVersion % 100) / 10);\n",
            "#endif\n",
            "  sProfileString += cTemp;\n",
            "\n",
            "  // Device count\n",
            "  sProfileString += \", NumDevs = \";\n",
            "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
            "  sprintf_s(cTemp, 10, \"%d\", deviceCount);\n",
            "#else\n",
            "  snprintf(cTemp, sizeof(cTemp), \"%d\", deviceCount);\n",
            "#endif\n",
            "  sProfileString += cTemp;\n",
            "  sProfileString += \"\\n\";\n",
            "  printf(\"%s\", sProfileString.c_str());\n",
            "\n",
            "  printf(\"Result = PASS\\n\");\n",
            "\n",
            "  // finish\n",
            "  exit(EXIT_SUCCESS);\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}